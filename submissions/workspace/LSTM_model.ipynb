{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Training for Oil, Water, and Gas Production Prediction\n",
    "\n",
    "This notebook imports necessary functions and libraries to preprocess data and train an LSTM model for oil, water, and gas production prediction. The model uses sequential data and aims to predict future production trends based on historical data.\n",
    "\n",
    "## Imported Libraries and Functions:\n",
    "\n",
    "- **`preprocess_data`**: Custom function from the `production_functions.py` script to prepare and preprocess data.\n",
    "- **`os`, `glob`**: Used to navigate directories and work with file paths.\n",
    "- **`pandas`, `numpy`**: Data handling libraries for loading, manipulating, and processing datasets.\n",
    "- **`Sequential`, `LSTM`, `Dense`**: Keras components to build and define the LSTM neural network.\n",
    "- **`train_test_split`**: Splits data into training and testing sets for model validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 13:17:16.168657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the functions from production_functions.py\n",
    "from function import preprocess_data\n",
    "\n",
    "# The rest of your code\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Multiple Reservoirs\n",
    "\n",
    "This code processes data from multiple reservoirs, prepares it for training an LSTM model, and scales the data for model input.\n",
    "\n",
    "## Key Preprocessing Steps:\n",
    "\n",
    "1. **Reservoir Folders**: The code retrieves a list of reservoir folders from the training dataset using the `glob` module.\n",
    "2. **File Paths**: For each reservoir, paths to the `state.csv` and `production.csv` files are constructed, representing static reservoir rock characteristics and day-by-day production data, respectively.\n",
    "3. **Data Loading and Processing**:\n",
    "   - The `state.csv` and `production.csv` files are loaded into DataFrames.\n",
    "   - The `Date` column in the production data is converted into a `datetime` format for consistency.\n",
    "   - The production data is **sorted by date** and **grouped by date**, calculating the **mean of production values** for each day to handle multiple entries for the same date.\n",
    "4. **Reservoir Rock Characteristics**:\n",
    "   - The mean values of the static rock characteristics (from the `state.csv` file) are calculated.\n",
    "   - These mean values are then **concatenated to the production data**, providing static characteristics alongside the dynamic production data for model input.\n",
    "5. **Lag Feature Calculation**:\n",
    "   - For the three production variables (oil, water, gas), **lag features** are created to incorporate the historical trends of production. Lag features are calculated for:\n",
    "     - **1-day lag**\n",
    "     - **3-day lag**\n",
    "     - **7-day lag**\n",
    "6. **Data Scaling**:\n",
    "   - The combined dataset (including lag features and reservoir rock characteristics) is **scaled** using `scalers_X` for input features and `scalers_y` for target variables, making the data suitable for model training.\n",
    "7. **Sequence Creation**:\n",
    "   - After scaling, the data is divided into **sequences of 7 time steps** (default value) to capture temporal dependencies. Each sequence consists of 7 consecutive days of production data along with the corresponding rock characteristics.\n",
    "   \n",
    "The final result of this preprocessing is a sequence of production and static features with a time step of 7, ready for training in the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X, all_y = [], []\n",
    "scalers_X, scalers_y = [], []\n",
    "\n",
    "# Get all reservoir folders\n",
    "reservoir_folders = glob.glob(os.path.join('../../dataset/training', 'Reservoir*'))\n",
    "\n",
    "for reservoir_folder in reservoir_folders:\n",
    "    # Extract reservoir_id from folder name\n",
    "    reservoir_id = os.path.basename(reservoir_folder)\n",
    "\n",
    "    # Define paths to status and production files\n",
    "    state_path = os.path.join(reservoir_folder, 'state.csv')\n",
    "    production_path = os.path.join(reservoir_folder, 'production.csv')\n",
    "    \n",
    "    # Check if files exist before processing\n",
    "    if os.path.exists(state_path) and os.path.exists(production_path):\n",
    "        # Load the datasets\n",
    "        status_df = pd.read_csv(state_path)\n",
    "        production_df = pd.read_csv(production_path)\n",
    "        # Ensure the 'date' column is in datetime format\n",
    "        production_df['Date'] = pd.to_datetime(production_df['Date'])\n",
    "\n",
    "        # Sort by date if not already sorted\n",
    "        production_df = production_df.sort_values(by='Date')\n",
    "        production_df = production_df.groupby(['Date']).mean().reset_index()\n",
    "        production_df = production_df.drop(columns='Date')\n",
    "\n",
    "        X_sequences, y_sequences, X_scaled, y_scaled, scaler_X, scaler_y = preprocess_data(status_df, production_df)\n",
    "        all_X.append(X_sequences)\n",
    "        all_y.append(y_sequences)\n",
    "        scalers_X.append(scaler_X)\n",
    "        scalers_y.append(scaler_y)\n",
    "\n",
    "X_combined = np.concatenate(all_X, axis=0)\n",
    "y_combined = np.concatenate(all_y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Training and Saving\n",
    "\n",
    "This code splits the preprocessed dataset into training and testing sets, builds an LSTM model, trains it on the training data, and then saves the trained model for future use.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Data Splitting**:\n",
    "   - The dataset is split into training (80%) and testing (20%) sets using the `train_test_split` function from `sklearn`. The `random_state=42` ensures reproducibility of the results.\n",
    "\n",
    "2. **LSTM Model Construction**:\n",
    "   - A sequential model is built using Keras. It includes:\n",
    "     - An LSTM layer with 50 units and `input_shape` set to `(7, 17)` (7 time steps, 17 features). \n",
    "     - A Dense output layer with 3 units corresponding to the 3 output variables (oil, water, and gas production).\n",
    "   - The model uses the **Adam** optimizer and **mean squared error** as the loss function.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - The model is trained for 50 epochs with a batch size of 32.\n",
    "   - `validation_split=0.2` means 20% of the training data is used for validation during training, allowing the model to monitor its performance on unseen data.\n",
    "\n",
    "4. **Model Saving**:\n",
    "   - After training, the model is saved to the file `LSTM_model.h5`, which can be loaded later for predictions or further training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimlai/project/dataWorks2024/dataworks/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0797 - val_loss: 0.0011\n",
      "Epoch 2/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8.2630e-04 - val_loss: 3.9371e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7088e-04 - val_loss: 2.3585e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4982e-04 - val_loss: 1.8856e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.9990e-04 - val_loss: 2.2398e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.1876e-04 - val_loss: 1.3570e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.9759e-04 - val_loss: 1.9263e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.6549e-04 - val_loss: 1.2575e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2964e-04 - val_loss: 1.0598e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1781e-04 - val_loss: 2.2947e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.8347e-04 - val_loss: 1.0329e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.6092e-04 - val_loss: 8.9015e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1449e-04 - val_loss: 1.1269e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 8.9379e-05 - val_loss: 7.0816e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2362e-04 - val_loss: 1.4478e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3267e-04 - val_loss: 7.3173e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.6505e-05 - val_loss: 9.0724e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2252e-04 - val_loss: 5.8576e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0010e-04 - val_loss: 1.6514e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5396e-04 - val_loss: 1.5805e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.1312e-05 - val_loss: 1.2935e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.4874e-05 - val_loss: 1.4714e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0937e-04 - val_loss: 1.0793e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 8.6161e-05 - val_loss: 7.5970e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0908e-04 - val_loss: 1.6935e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.8375e-05 - val_loss: 9.3166e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.7392e-05 - val_loss: 1.0045e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1768e-04 - val_loss: 5.5632e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.4217e-05 - val_loss: 6.7032e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8766e-05 - val_loss: 3.7596e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7913e-04 - val_loss: 1.0706e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.7332e-05 - val_loss: 4.6512e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.0955e-05 - val_loss: 4.0396e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1109e-04 - val_loss: 4.7816e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.8693e-05 - val_loss: 6.3286e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.3123e-05 - val_loss: 5.0783e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.0587e-05 - val_loss: 9.4985e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.0208e-04 - val_loss: 3.6498e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.5223e-05 - val_loss: 8.0822e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4133e-04 - val_loss: 9.3519e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.5747e-05 - val_loss: 5.8484e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.5968e-05 - val_loss: 2.7876e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.9878e-05 - val_loss: 5.9333e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.9154e-05 - val_loss: 6.8993e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.6146e-05 - val_loss: 5.8739e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.3668e-05 - val_loss: 9.0773e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.3223e-05 - val_loss: 2.5640e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5.1445e-05 - val_loss: 3.7105e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.7118e-05 - val_loss: 3.3362e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.3768e-05 - val_loss: 2.3658e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=False, input_shape=(7, 17)))\n",
    "model.add(Dense(3))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save('models/LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This section evaluates the performance of the trained LSTM model on the test dataset using several metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and the R² score. \n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Model Prediction**:\n",
    "   - The model predicts production values (`y_pred`) using the test data (`X_test`).\n",
    "   - Both `y_test` and `y_pred` are reshaped to flatten them so they can be compared.\n",
    "\n",
    "2. **Inverse Scaling**:\n",
    "   - Since the target variables (`y_test` and `y_pred`) were scaled during preprocessing, they are transformed back to their original scale using the inverse transformation from the `scalers_y`.\n",
    "   - This allows for error metrics to be computed in the original production units (e.g., barrels of oil, water, gas).\n",
    "\n",
    "3. **Error Metrics Calculation**:\n",
    "   - **Mean Squared Error (MSE)**: Measures the average of the squared differences between actual and predicted values. Lower values indicate better model performance.\n",
    "   - **Mean Absolute Error (MAE)**: Measures the average of the absolute differences between actual and predicted values. It is a direct interpretation of error magnitude.\n",
    "   - **R² Score (Coefficient of Determination)**: Evaluates how well the predicted values fit the actual values. An R² score of 1 indicates a perfect fit, while 0 or negative values indicate poor performance.\n",
    "\n",
    "4. **Print Results**:\n",
    "   - The computed metrics are printed to provide insights into how well the model performed on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Mean Squared Error: 22837023611.819126\n",
      "Mean Absolute Error: 82605.80888133099\n",
      "R² Score: 0.9999580846302782\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_flattened = y_test.reshape(-1, y_test.shape[-1])\n",
    "y_pred_flattened = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "\n",
    "y_test_rescaled = scalers_y[0].inverse_transform(y_test_flattened)\n",
    "y_pred_rescaled = scalers_y[0].inverse_transform(y_pred_flattened)\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'R² Score: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
